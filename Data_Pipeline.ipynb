{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Collecting Data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import os\n",
    "import json\n",
    "\n",
    "##############################################\n",
    "# FUNCTIONS\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login to Twitter API\n",
    "def oauth_login():\n",
    "    # Access tokens from Twitter deveopers account\n",
    "    CONSUMER_KEY = 'cmp08UIimneHjcnbzAJDQFoIs'\n",
    "    CONSUMER_SECRET = 'fqfMzY9GaoXETfwO8XSmi7Ykau8GmAyX5xNamLmGVmR5WgmTwc'\n",
    "    OAUTH_TOKEN = '839511169807302656-PImkKuBn8x4elz56BmRpqXYJHTbF1J8'\n",
    "    OAUTH_TOKEN_SECRET = 'vc2vOrFnz8Sf17eR1fn4HUUn2yOGff3d7ZA7VxLElTHyi'\n",
    "        \n",
    "    # Send tokens to Twitter\n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    # Link to Twitter api\n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "\n",
    "    return twitter_api\n",
    "\n",
    "def twitter_search(twitter_api, q, max_results=1000, **kw):\n",
    "    # See http://bit.ly/2QyGz0P and https://bit.ly/2QyGz0P for\n",
    "    # details on advanced search criteria that may be useful for\n",
    "    # keyword arguments\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
    "    # JM: should count be same as max_results?\n",
    "    search_results = twitter_api.search.tweets(q=q, count=1000, **kw)\n",
    "\n",
    "    # Get statuses. \n",
    "    statuses = search_results['statuses']\n",
    "\n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://developer.twitter.com/en/docs/basics/rate-limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "\n",
    "    # Enforce a reasonable limit.\n",
    "    max_results = min(1000, max_results)\n",
    "\n",
    "    # JM: not sure what this is doing? Whatever it is, do it 10 times.\n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=')\n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "\n",
    "        if len(statuses) > max_results:\n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "def appendTweets(filename):\n",
    "    # This allows you to create a running list of the tweets you gather \n",
    "    # Note, if results does not contain new data you will be appending \n",
    "    # copies of the same tweets.\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r+') as file:\n",
    "            # First we load existing data into a dict.\n",
    "            file_data = json.load(file)\n",
    "            # Join new_data with file_data\n",
    "            file_data += results\n",
    "            # Sets file's current position at offset.\n",
    "            file.seek(0)\n",
    "            # convert back to json.\n",
    "            json.dump(file_data, file, indent = 1)\n",
    "    else:\n",
    "        with open(filename, 'w+') as file:\n",
    "            # convert back to json.\n",
    "            json.dump(results, file, indent = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage. Fetch Tweets!\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# You can change this to be any string you like.\n",
    "# lang='en' returns only (self identified) english tweets\n",
    "q = '@MzBerryThrows' # Gwen Berry, Olympic Hammer Thrower\n",
    "results = twitter_search(twitter_api, q, tweet_mode='extended', lang='en', max_results=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of tweets\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store/append downloaded tweets to file\n",
    "filename = 'tweets_%s.json'%q\n",
    "appendTweets(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Organizing your data using MongoDB  \n",
    "## Reading data into MongoDB  \n",
    "Note: You will need to install \"pymongo\" using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tweets from file\n",
    "\n",
    "# Likely same q as above\n",
    "q = '@MzBerryThrows'\n",
    "filename = 'tweets_%s.json'%q\n",
    "\n",
    "# Read in file\n",
    "results = json.load(open(filename, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of tweets\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open connection to db, drop tweets into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based upon example 9-7 in\n",
    "# Mining the Soocial Web, Chapter 9\n",
    "  \n",
    "# The connection string for a remote hosted mongodb running on MongoDB atlas\n",
    "# NOTE!!!!! this requires the \"dnspython\" package to be installed. See the following for details:\n",
    "# https://pymongo.readthedocs.io/en/stable/installation.html\n",
    "#client = pymongo.MongoClient(\"mongodb+srv://test:epsabre@cluster0.fup2q.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "\n",
    "# A local mongodb running on your personal machine installed following the documentation:\n",
    "# https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/ \n",
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "\n",
    "# Get a reference to a particular database\n",
    "db = client['twitter']\n",
    "\n",
    "# Reference a particular collection in the database\n",
    "coll = db['statuses_'+q]\n",
    "\n",
    "# Clear any old data out of the collection\n",
    "# Warning: Make sure you want to do this! \n",
    "#coll.drop()\n",
    "    \n",
    "# Perform a bulk insert and return the IDs   \n",
    "_ = coll.insert_many(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
