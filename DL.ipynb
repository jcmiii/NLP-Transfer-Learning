{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continuing with movie reviews from the v2.0 polarity dataset at http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies tagged as positive or negative.\n",
    "* This notebook builds upon the turtorial \"Working With Text Data\" found at   \n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* You may find this information about deep learning helpful:   \n",
    "https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Numpy (www.numpy.org) \n",
    "* Matplotlib (matplotlib.org) \n",
    "* Scikit-learn (scikit-learn.org) \n",
    "* Pytorch (www.pytorch.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the movie review data, create TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Training data folder must be passed as first argument.\n",
    "# Note the 'txt_sentoken' contains two folders: 'neg' and 'pos'\n",
    "# The 'neg' ('pos') folder contains text files of negative (positive) movie reviews\n",
    "movie_reviews_data_folder = 'txt_sentoken'\n",
    "print(\"loading text...\")\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset in training and test sets:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TfidVectorizer to create features for the text documents\n",
    "features = TfidfVectorizer(min_df=3, max_df=0.95, ngram_range=(1,1))\n",
    "X_train=features.fit_transform(docs_train)\n",
    "X_test=features.transform(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impliment PCA to project original features into lower dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (which relies on Singular Value Decomposition, or SVD) reduces the number of features, which in turn reduces the run time for our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847397146331857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "svd.fit(X_train)\n",
    "print(sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = svd.transform(X_train)\n",
    "X_test = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Multi-Layer Perceptron classifier using the movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate Pytorch library to implement a simple MLP classifier.   \n",
    "Note you will need to install Pytorch using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()       # equivalent to super().__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 500)\n",
    "        self.fc2 = nn.Linear(500, 400)\n",
    "        self.fc3 = nn.Linear(400, 300)\n",
    "        self.fc4 = nn.Linear(300, 200)\n",
    "        self.fc5 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # relu = rectified linear\n",
    "        # See link for other options: https://pytorch.org/docs/stable/nn.functional.html\n",
    "        intermediate_vector1 = F.relu(self.fc1(input))\n",
    "        intermediate_vector2 = F.relu(self.fc2(intermediate_vector1))\n",
    "        intermediate_vector3 = F.relu(self.fc3(intermediate_vector2))\n",
    "        intermediate_vector4 = F.relu(self.fc4(intermediate_vector3))\n",
    "        prediction_vector = torch.sigmoid(self.fc5(intermediate_vector4))\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance\n",
    "mlp = MLPClassifier() \n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(mlp.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 completed.\n",
      "Epoch 200 completed.\n",
      "Epoch 300 completed.\n",
      "Epoch 400 completed.\n",
      "Epoch 500 completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "y_train = y_train.reshape(-1,1)\n",
    "\n",
    "for i in range(1, num_epochs+1):\n",
    "    # Step 1. set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 2. compute output\n",
    "    y_pred = mlp(X_train.float())\n",
    "\n",
    "    # Step 3. compute loss\n",
    "    loss = loss_func(y_pred, torch.tensor(y_train, dtype=torch.float))\n",
    "\n",
    "    # Step 4. use loss to produce gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 5. use optimizer to take gradient step\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Epoch ' + str(i)  + ' completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = mlp(X_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = (torch.round(y_predicted)).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.scatter(range(len(y_predicted)), y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main point of the project. Apply transfer learning to an NN trained on movie reviews. How will the NN perform on tweets? It turns out, not so well. However, adding only 100 labeled tweets to the movie database of 2100 reviews and retraining the network improved the performance of the NN remarkably.  \n",
    "\n",
    "A sample of 100 tweets (regarding Gwen Berry), labeled by humans, yielded a point estimate of .31 for the proportion of positive tweets. An NN trained on movie reviews only categorized 70% of the tweets as positive. By adding the 100 labled tweets to the training set of 2100 movie reviews the retraining network categorized 28% of the tweets as positive; very close to the point estimate (95% CI [0.23, 0.41])\n",
    "\n",
    "1. Import data (tweets + metadata) from MongoDB\n",
    "1. Extract tweet full_text from data \n",
    "1. Use the trained network to classify tweets using (simple) transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '@MzBerryThrows'\n",
    "    \n",
    "# The connection string for a remote hosted mongodb running on MongoDB atlas\n",
    "# client = pymongo.MongoClient(\"mongodb+srv://test:epsabre@cluster0.fup2q.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "\n",
    "# A local mongodb running on your personal machine installed from using the documentation:\n",
    "#    https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/ \n",
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "\n",
    "# Get a reference to a particular database    \n",
    "db = client['twitter']\n",
    "    \n",
    "# Reference a particular collection in the database\n",
    "coll = db['statuses_'+q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a search in MongoDB\n",
    "\n",
    "# This pulls all records, but not all records have full text.\n",
    "#cursor = coll.find({})\n",
    "\n",
    "# This should pull all records with full text, as all tweets should \n",
    "# contain '@' in the full_text field given the twitter search\n",
    "cursor = coll.find({ 'full_text' : {'$regex': '.*@.*'} })\n",
    "\n",
    "# Count queried tweets, i.e. tweets w/ full text\n",
    "n=0\n",
    "for tweet in cursor:\n",
    "    n = n + 1\n",
    "print('Number of tweets: ', n)\n",
    "\n",
    "#list(tweets.find({'entities.hashtags.text': {\"$ne\":None}}))\n",
    "# Need to reinitialize cursor before iterating through it again.\n",
    "cursor = coll.find({ 'full_text' : {'$regex': '.*@.*'} })\n",
    "X_tweet = []\n",
    "for tweet in cursor:\n",
    "    X_tweet.append(tweet['full_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweet_features = features.transform(X_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small visualziation to see if the movie review features and the tweets have at least some overlap.  \n",
    "Not much!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.spy(X_tweet_features[:,:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our pretrained preprocessing chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweet_features_projected = svd.transform(X_tweet_features)\n",
    "X_tweet_features_projected = torch.from_numpy(X_tweet_features_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our pretrained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tweet_predicted = mlp(X_tweet_features_projected.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we get something reasonable?  \n",
    "Some clearly negative tweets receive high scores (> 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(X_tweet[i])\n",
    "    print(y_tweet_predicted[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Ratio\n",
    "#### Some ways to measure the sentiment of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - this pulls the number out of the tensor\n",
    "#len(y_tweet_predicted)\n",
    "y_tweet_predicted[0].data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways we can do this. \n",
    "# 1. Require all tweets to be either positive or negative\n",
    "# 2. classify tweets as pos, neg, neutral\n",
    "\n",
    "# Parameters\n",
    "pos_lo_bnd = .66\n",
    "mid = .5\n",
    "neg_hi_bnd = .33\n",
    "\n",
    "\n",
    "# Initialize\n",
    "pos_sent = 0\n",
    "pos_neut_sent = 0\n",
    "neg_neut_sent = 0\n",
    "neg_sent = 0\n",
    "\n",
    "pos_cnt = 0\n",
    "pos_neut_cnt = 0\n",
    "neg_neut_cnt = 0\n",
    "neg_cnt = 0\n",
    "\n",
    "for y in y_tweet_predicted:\n",
    "    if (y.data.numpy()[0] < neg_hi_bnd):\n",
    "        neg_sent += 2 * y.data.numpy()[0] - 1  # scale to [-1,1]\n",
    "        neg_cnt += 1\n",
    "    elif (y.data.numpy()[0] < mid):\n",
    "        neg_neut_sent += 2 * y.data.numpy()[0] - 1\n",
    "        neg_neut_cnt += 1\n",
    "    elif (y.data.numpy()[0] < pos_lo_bnd):\n",
    "        pos_neut_sent += 2 * y.data.numpy()[0] - 1 \n",
    "        pos_neut_cnt += 1\n",
    "    else:\n",
    "        pos_sent += 2 * y.data.numpy()[0] - 1\n",
    "        pos_cnt += 1\n",
    "        \n",
    "# If all tweets are required to be either pos or neg\n",
    "sent_ratio = -(pos_sent + pos_neut_sent) / (neg_sent + neg_neut_sent)\n",
    "cnt_ratio = (pos_cnt + pos_neut_cnt) / (neg_cnt + neg_neut_cnt)\n",
    "print(\"sentiment ratio w/ no neutral tweets: \", sent_ratio)\n",
    "print(\"count ratio: \", cnt_ratio)\n",
    "print(\"positive tweets: \", pos_cnt + pos_neut_cnt)\n",
    "print(\"negative tweets: \", neg_cnt + neg_neut_cnt)\n",
    "\n",
    "# If we allow for neutral tweets\n",
    "sent_ratio = -(pos_sent / neg_sent)\n",
    "cnt_ratio = pos_cnt / neg_cnt\n",
    "print(\"sentiment ratio allowing for neutral tweets: \", sent_ratio)\n",
    "print(\"count ratio: \", cnt_ratio)\n",
    "print(\"positive tweets: \", pos_cnt)\n",
    "print(\"neutral tweets: \", pos_neut_cnt + neg_neut_cnt)\n",
    "print(\"negative tweets: \", neg_cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
